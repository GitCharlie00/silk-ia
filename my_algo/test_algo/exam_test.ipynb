{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test del mio algoritmo sul dataset dell'ammissione al college"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Importo le librerie necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Sigmoid o Logistic function\n",
    "Per effettuare la classificazione serve una funzione con codominio [0,1]. Infatti la predizione si baserà sulla probabilità della stima di appartenere ad una o all'altra classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    den = 1 + np.exp(-z)\n",
    "    sigm = 1.0/den\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Funzione di ipotesi\n",
    "Per approssimare una probabilità, la funzione di costo sarà basta sulla funzione logisitca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp(W,X):\n",
    "    param = np.dot(W,X.T)\n",
    "    return logistic(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Funzione di costo\n",
    "Da massimizzare nello step di ottimizzazione. IN particolare il logaritmo della funzione di coston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(W,X,Y):\n",
    "    m = X.shape[0]\n",
    "    h = hyp(W,X)\n",
    "    log_h = np.log(h)\n",
    "    log_one_h = np.log(1-h)\n",
    "    l_cost = float((-1.0/m) * ((np.dot(log_h,Y)) + (np.dot(log_one_h,(1-Y)))))\n",
    "    return l_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Carico e preparo i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carico il dataset\n",
    "path = './ex2data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=[\"Ex1\",\"Ex2\",\"Cl\"])\n",
    "\n",
    "# Creo train e test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[[\"Ex1\",\"Ex2\"]], data[['Cl']], test_size=0.13)\n",
    "\n",
    "m_tr, n_tr = X_train.shape\n",
    "X_train = np.concatenate((np.ones((m_tr,1)), X_train), axis=1)\n",
    "n_tr +=1\n",
    "\n",
    "m_te, n_te = X_test.shape\n",
    "X_test = np.concatenate((np.ones((m_te,1)), X_test), axis=1)\n",
    "n_te += 1\n",
    "\n",
    "W = np.array(np.zeros((1,n_te)))\n",
    "#W = np.matrix(np.random.randn((n_te)))\n",
    "\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "\n",
    "# Normalizzo i dati\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "#cost(W,X_train,Y_train)\n",
    "X_train.shape, Y_train.shape, W.shape\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Fit dell'algoritmo con gradient ascent\n",
    "Cerco il punto di massimo della funzione di costo per trovare i pesi ottimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "0.010264215354685247\n",
      "0.022949480752735618\n",
      "0.033455584740426425\n",
      "0.04080272910367244\n",
      "0.04445910530831099\n",
      "0.04448052433150834\n",
      "0.041492497979092546\n",
      "0.036477457455924844\n",
      "0.03046794306464523\n",
      "0.02430386176791588\n",
      "0.01853453924976073\n",
      "0.013440681644504238\n",
      "0.009110104772303063\n",
      "0.005516323897164999\n",
      "0.002577837814809081\n",
      "0.00019526198533376116\n",
      "-0.0017287017233652557\n",
      "-0.003280314652713723\n",
      "-0.004532059650734999\n",
      "-0.0055429895202230295\n",
      "-0.0063603687332557945\n",
      "-0.007021669044733947\n",
      "-0.007556479062908739\n",
      "-0.007988155963630617\n",
      "-0.008335180368336192\n",
      "-0.008612234473379943\n",
      "-0.008831044834770096\n",
      "-0.009001034594406876\n",
      "-0.009129825732145158\n",
      "-0.009223625256038703\n",
      "-0.009287522442206764\n",
      "-0.009325718242332748\n",
      "-0.009341703053224126\n",
      "-0.009338395154023627\n",
      "-0.00931824911645307\n",
      "-0.009283341211711149\n",
      "-0.009235437116968226\n",
      "-0.009176045932774213\n",
      "-0.009106463554959698\n",
      "-0.009027807719406256\n",
      "-0.008941046493807225\n",
      "-0.008847021580907999\n",
      "-0.008746467488395582\n",
      "-0.008640027385990567\n",
      "-0.008528266291570996\n",
      "-0.008411682091282402\n",
      "-0.008290714793246834\n",
      "-0.008165754332956743\n",
      "-0.00803714718497639\n",
      "-0.00790520198594058\n",
      "-0.007770194334756342\n",
      "-0.007632370904995467\n",
      "-0.007491952979877592\n",
      "-0.007349139500558599\n",
      "-0.007204109702620309\n",
      "-0.007057025402879891\n",
      "-0.006908032988240609\n",
      "-0.006757265149845093\n",
      "-0.006604842398846067\n",
      "-0.006450874394373418\n",
      "-0.006295461109579015\n",
      "-0.006138693857688726\n",
      "-0.005980656196731604\n",
      "-0.005821424728888358\n",
      "-0.005661069808082453\n",
      "-0.0054996561675357825\n",
      "-0.005337243477348652\n",
      "-0.005173886840793895\n",
      "-0.005009637236843223\n",
      "-0.004844541915428935\n",
      "-0.004678644751099914\n",
      "-0.004511986559993408\n",
      "-0.004344605384413502\n",
      "-0.0041765367487704985\n",
      "-0.004007813890163803\n",
      "-0.0038384679664968946\n",
      "-0.003668528244647473\n",
      "-0.003498022270940093\n",
      "-0.0033269760258751724\n",
      "-0.0031554140648696327\n",
      "-0.002983359646542727\n",
      "-0.002810834849924282\n",
      "-0.002637860681800497\n",
      "-0.0024644571752875377\n",
      "-0.0022906434806039266\n",
      "-0.002116437948917471\n",
      "-0.001941858210045888\n",
      "-0.0017669212447291027\n",
      "-0.00159164345209728\n",
      "-0.0014160407129470975\n",
      "-0.0012401284493234188\n",
      "-0.0010639216809320562\n",
      "-0.0008874350788131702\n",
      "-0.0007106830167058487\n",
      "-0.000533679620505656\n",
      "-0.0003564388161765297\n",
      "-0.00017897437648917158\n",
      "-1.2999669156688043e-06\n",
      "0.00017657080896771316\n",
      "0.000354624364252043\n",
      "0.0005328470833099175\n",
      "0.0007112252759111648\n",
      "0.0008897451305169835\n",
      "0.0010683926663920262\n",
      "0.0012471536841817121\n",
      "0.0014260137145654106\n",
      "0.00160495796459037\n",
      "0.0017839712612491843\n",
      "0.001963037991837724\n",
      "0.002142142040585937\n",
      "0.002321266721013293\n",
      "0.0025003947034022422\n",
      "0.002679507936716563\n",
      "0.002858587564233517\n",
      "0.0030376138320594714\n",
      "0.0032165659896068455\n",
      "0.0033954221810122975\n",
      "0.003574159326332982\n",
      "0.003752752991216579\n",
      "0.003931177243586159\n",
      "0.004109404495664326\n",
      "0.0042874053294647\n",
      "0.004465148303596789\n",
      "0.004642599738957753\n",
      "0.0048197234805215095\n",
      "0.004996480632051381\n",
      "0.005172829260093104\n",
      "0.005348724063068633\n",
      "0.0055241160006731516\n",
      "0.0056989518780364845\n",
      "0.005873173878256255\n",
      "0.006046719035921133\n",
      "0.006219518643045263\n",
      "0.006391497577460603\n",
      "0.00656257354207046\n",
      "0.006732656201426268\n",
      "0.006901646199787059\n",
      "0.007069434042075717\n",
      "0.0072358988158695015\n",
      "0.007400906728630585\n",
      "0.007564309429666793\n",
      "0.007725942080602066\n",
      "0.007885621131236031\n",
      "0.008043141749270122\n",
      "0.008198274842101783\n",
      "0.008350763596302602\n",
      "0.008500319444812221\n",
      "0.008646617352632613\n",
      "0.008789290287692375\n",
      "0.008927922713368952\n",
      "0.009062042900969236\n",
      "0.009191113811889973\n",
      "0.009314522236913314\n",
      "0.009431565799660757\n",
      "0.009541437326425595\n",
      "0.009643205946974154\n",
      "0.009735794108453377\n",
      "0.00981794944053832\n",
      "0.009888210080630067\n",
      "0.009944861619311407\n",
      "0.009985883209702673\n",
      "0.010008879529498194\n",
      "0.01001099408941114\n",
      "0.009988797698438912\n",
      "0.009938143509431785\n",
      "0.009853976665218034\n",
      "0.009730081694279802\n",
      "0.00955874381943178\n",
      "0.009330290334955749\n",
      "0.009032463938700608\n",
      "0.008649559776185312\n",
      "0.008161230107042339\n",
      "0.007540823300404009\n",
      "0.006753077251636652\n",
      "0.00575093621577627\n",
      "0.004471222113682782\n",
      "0.002828913458892024\n",
      "0.0007099763349566746\n",
      "-0.0020367157905160638\n",
      "-0.005606394598617559\n",
      "-0.010239312520742339\n",
      "-0.01620489941423031\n",
      "-0.023749985311577793\n",
      "-0.032983903496721256\n",
      "-0.043679053296782844\n",
      "-0.05501277356955392\n",
      "-0.06538808297944965\n",
      "-0.07258852507378499\n",
      "-0.0744269614904085\n",
      "-0.06959580225386208\n",
      "-0.058098185552977744\n",
      "-0.041034557219831624\n",
      "-0.020109937193985572\n",
      "0.002741669817951986\n",
      "0.02551569065785353\n",
      "0.046167750648482775\n",
      "0.06265793064389036\n",
      "0.07316053339338768\n",
      "0.07654146279917073\n",
      "0.07295756639372053\n",
      "0.0640560013881486\n",
      "0.05236028325269232\n",
      "0.04023630279572232\n",
      "0.029226245629846326\n",
      "0.019998516382289555\n",
      "0.012627560514038016\n",
      "0.006895941375167802\n",
      "0.0024956934315779056\n",
      "-0.0008703789588672683\n",
      "-0.003450954895390934\n",
      "-0.005440182322715759\n",
      "-0.006984313754348603\n",
      "-0.008191630903889524\n",
      "-0.00914181050539159\n",
      "-0.009893537109229467\n",
      "-0.010490276526786635\n",
      "-0.010964500488471252\n",
      "-0.011340721955059752\n",
      "-0.011637658552279329\n",
      "-0.01186977237985215\n",
      "-0.012048369238160272\n",
      "-0.012182388079444273\n",
      "-0.01227897261168176\n",
      "-0.012343889143013431\n",
      "-0.01238183523701869\n",
      "-0.012396670218701333\n",
      "-0.012391589237173928\n",
      "-0.012369256156715303\n",
      "-0.012331906102288626\n",
      "-0.012281425400472501\n",
      "-0.01221941450277586\n",
      "-0.012147237963465729\n",
      "-0.012066064469998472\n",
      "-0.011976899155858534\n",
      "-0.011880609870945746\n",
      "-0.011777948680363193\n",
      "-0.011669569564921933\n",
      "-0.01155604307559499\n",
      "-0.011437868528310302\n",
      "-0.011315484199911352\n",
      "-0.011189275890208794\n",
      "-0.011059584141143719\n",
      "-0.010926710346709223\n",
      "-0.010790921942339882\n",
      "-0.010652456827081824\n",
      "-0.010511527143739152\n",
      "-0.010368322519746953\n",
      "-0.010223012853483149\n",
      "-0.010075750716152854\n",
      "-0.009926673427554933\n",
      "-0.00977590485436719\n",
      "-0.009623556971686598\n",
      "-0.00946973122203798\n",
      "-0.009314519700682533\n",
      "-0.009158006191590351\n",
      "-0.009000267074737889\n",
      "-0.008841372122257662\n",
      "-0.008681385198422653\n",
      "-0.00852036487619423\n",
      "-0.00835836498129816\n",
      "-0.008195435073177104\n",
      "-0.008031620870894374\n",
      "-0.007866964630930506\n",
      "-0.007701505482867854\n",
      "-0.007535279728142497\n",
      "-0.0073683211063703125\n",
      "-0.007200661033135658\n",
      "-0.007032328812639266\n",
      "-0.00686335182816844\n",
      "-0.00669375571295816\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cschi\\AppData\\Local\\Temp\\ipykernel_16492\\116237179.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  log_one_h = np.log(1-h)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[14.05884715, 14.61477251, 14.16826208]]), 271)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_ascent(X,W,Y,alpha,stop):\n",
    "    m,n = X.shape\n",
    "    cost_old = np.inf\n",
    "    cost_new = cost(W,X,Y)\n",
    "    print(cost_old - cost(W,X,Y))\n",
    "    sum = 0\n",
    "    iter = 0\n",
    "\n",
    "    while(abs(cost_old-cost_new) > stop):\n",
    "        for j in range(0,n):\n",
    "            for i in range(0,m):\n",
    "                sum += (Y[i][0] - hyp(W,X[i])) * X[i][j]\n",
    "            W[0][j] = W[0][j] + (alpha/m) * sum\n",
    "        cost_old = cost_new\n",
    "        cost_new = cost(W,X,Y)\n",
    "        print(cost_old - cost(W,X,Y))\n",
    "        iter += 1\n",
    "\n",
    "    return W, iter  \n",
    "\n",
    "gradient_ascent(X_train,W,Y_train,0.05,0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Model Assestement\n",
    "Train accurancy dell'esercitazione: 0,89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8505747126436781, 0.8461538461538463)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prediction(W,X,Y):\n",
    "    m = X.shape[0]\n",
    "    Y_hat = hyp(X, W) > 0.5\n",
    "    accuracy = 1.0/m * np.sum(Y == Y_hat)\n",
    "    return accuracy, Y_hat\n",
    "\n",
    "prediction(W,X_train,Y_train)[0],prediction(W,X_test,Y_test)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "702afc27125c4b44e8d6e4d91e2be92deaff276ecbbea6cc8c365578c4a94287"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione di un classificatore con Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Importo le librerie necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Sigmoid o Logistic function\n",
    "Per effettuare la classificazione serve una funzione con codominio [0,1]. Infatti la predizione si baserà sulla probabilità della stima di appartenere ad una o all'altra classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    den = 1 + np.exp(-z)\n",
    "    sigm = 1.0/den\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Funzione di ipotesi\n",
    "Per approssimare una probabilità, la funzione di costo sarà basta sulla funzione logisitca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp(W,X):\n",
    "    param = np.dot(W,X.T)\n",
    "    return logistic(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Funzione di costo\n",
    "Da massimizzare nello step di ottimizzazione. IN particolare il logaritmo della funzione di coston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(W,X,Y):\n",
    "    m = X.shape[0]\n",
    "    h = hyp(W,X)\n",
    "    log_h = np.log(h)\n",
    "    log_one_h = np.log(1-h)\n",
    "    l_cost = float((-1.0/m) * ((np.dot(log_h,Y)) + (np.dot(log_one_h,(1-Y)))))\n",
    "    return l_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Carico e preparo i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carico il dataset\n",
    "path = '../color_extr/data.csv'\n",
    "data = pd.read_csv(path, usecols = [i for i in range(5)])\n",
    "\n",
    "# Creo train e test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[['presenza_foglie','assenza_foglie','presenza_bachi_sfondo','assenza_bachi_sfondo']], data[['classificazione']], test_size=0.13)\n",
    "\n",
    "m_tr, n_tr = X_train.shape\n",
    "X_train = np.concatenate((np.ones((m_tr,1)), X_train), axis=1)\n",
    "n_tr +=1\n",
    "\n",
    "m_te, n_te = X_test.shape\n",
    "X_test = np.concatenate((np.ones((m_te,1)), X_test), axis=1)\n",
    "n_te += 1\n",
    "\n",
    "W = np.array(np.zeros((1,n_te)))\n",
    "#W = np.matrix(np.random.randn((n_te)))\n",
    "\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "\n",
    "# Normalizzo i dati\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "#cost(W,X_train,Y_train)\n",
    "X_train.shape\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Fit dell'algoritmo con gradient ascent\n",
    "Cerco il punto di massimo della funzione di costo per trovare i pesi ottimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "8.253715535866402e-05\n",
      "8.228280649599795e-05\n",
      "8.202969284842876e-05\n",
      "8.17778062346397e-05\n",
      "8.152713854198135e-05\n",
      "8.12776817255556e-05\n",
      "8.102942780766065e-05\n",
      "8.07823688773468e-05\n",
      "8.053649708913979e-05\n",
      "8.029180466309627e-05\n",
      "8.004828388358254e-05\n",
      "7.980592709885825e-05\n",
      "7.956472672060455e-05\n",
      "7.932467522281383e-05\n",
      "7.908576514173427e-05\n",
      "7.88479890748428e-05\n",
      "7.861133968045664e-05\n",
      "7.837580967692825e-05\n",
      "7.814139184225688e-05\n",
      "7.790807901339458e-05\n",
      "7.767586408580218e-05\n",
      "7.744474001245005e-05\n",
      "7.721469980379037e-05\n",
      "7.698573652700769e-05\n",
      "7.675784330521407e-05\n",
      "7.653101331725476e-05\n",
      "7.630523979688941e-05\n",
      "7.608051603245902e-05\n",
      "7.585683536628918e-05\n",
      "7.563419119406556e-05\n",
      "7.541257696444537e-05\n",
      "7.519198617848832e-05\n",
      "7.497241238922647e-05\n",
      "7.475384920094252e-05\n",
      "7.453629026896169e-05\n",
      "7.431972929901332e-05\n",
      "7.410416004667575e-05\n",
      "7.388957631705717e-05\n",
      "7.367597196419884e-05\n",
      "7.346334089081141e-05\n",
      "7.325167704730351e-05\n",
      "7.304097443210089e-05\n",
      "7.28312270904391e-05\n",
      "7.262242911450223e-05\n",
      "7.241457464243761e-05\n",
      "7.22076578585501e-05\n",
      "7.200167299215021e-05\n",
      "7.179661431783169e-05\n",
      "7.159247615448616e-05\n",
      "7.138925286520603e-05\n",
      "7.118693885686811e-05\n",
      "7.09855285794675e-05\n",
      "7.078501652588165e-05\n",
      "7.058539723164836e-05\n",
      "7.038666527423021e-05\n",
      "7.018881527284804e-05\n",
      "6.999184188799523e-05\n",
      "6.979573982113241e-05\n",
      "6.960050381418781e-05\n",
      "6.940612864930751e-05\n",
      "6.921260914841132e-05\n",
      "6.9019940172943e-05\n",
      "6.882811662323185e-05\n",
      "6.863713343849276e-05\n",
      "6.84469855961739e-05\n",
      "6.82576681117486e-05\n",
      "6.806917603839613e-05\n",
      "6.788150446651597e-05\n",
      "6.769464852358908e-05\n",
      "6.750860337360887e-05\n",
      "6.732336421697016e-05\n",
      "6.713892628992801e-05\n",
      "6.695528486455604e-05\n",
      "6.677243524798315e-05\n",
      "6.659037278265723e-05\n",
      "6.640909284547081e-05\n",
      "6.622859084771948e-05\n",
      "6.604886223490758e-05\n",
      "6.586990248616531e-05\n",
      "6.569170711409611e-05\n",
      "6.551427166459622e-05\n",
      "6.533759171613307e-05\n",
      "6.516166288005054e-05\n",
      "6.498648079980573e-05\n",
      "6.481204115085792e-05\n",
      "6.46383396404604e-05\n",
      "6.446537200721636e-05\n",
      "6.429313402077363e-05\n",
      "6.412162148189404e-05\n",
      "6.395083022178727e-05\n",
      "6.378075610204148e-05\n",
      "6.36113950141376e-05\n",
      "6.344274287971297e-05\n",
      "6.327479564968708e-05\n",
      "6.310754930408113e-05\n",
      "6.294099985246215e-05\n",
      "6.277514333269396e-05\n",
      "6.260997581142291e-05\n",
      "6.244549338339789e-05\n",
      "6.228169217151192e-05\n",
      "6.21185683264136e-05\n",
      "6.195611802629897e-05\n",
      "6.179433747652285e-05\n",
      "6.163322290976547e-05\n",
      "6.147277058535239e-05\n",
      "6.131297678926839e-05\n",
      "6.115383783387995e-05\n",
      "6.099535005767154e-05\n",
      "6.083750982502356e-05\n",
      "6.068031352622627e-05\n",
      "6.052375757668871e-05\n",
      "6.03678384173828e-05\n",
      "6.02125525142605e-05\n",
      "6.005789635787906e-05\n",
      "5.990386646395618e-05\n",
      "5.975045937196832e-05\n",
      "5.95976716461083e-05\n",
      "5.9445499874216656e-05\n",
      "5.929394066833682e-05\n",
      "5.914299066361872e-05\n",
      "5.899264651905434e-05\n",
      "5.884290491653399e-05\n",
      "5.869376256116554e-05\n",
      "5.8545216180774795e-05\n",
      "5.8397262525683447e-05\n",
      "5.8249898369028275e-05\n",
      "5.810312050574806e-05\n",
      "5.79569257532081e-05\n",
      "5.781131095038139e-05\n",
      "5.7666272958084575e-05\n",
      "5.75218086586865e-05\n",
      "5.73779149557474e-05\n",
      "5.7234588774102146e-05\n",
      "5.709182705958271e-05\n",
      "5.694962677875448e-05\n",
      "5.680798491883299e-05\n",
      "5.666689848765616e-05\n",
      "5.6526364513115324e-05\n",
      "5.638638004351604e-05\n",
      "5.624694214685644e-05\n",
      "5.610804791120194e-05\n",
      "5.59696944440885e-05\n",
      "5.5831878872633633e-05\n",
      "5.569459834314783e-05\n",
      "5.555785002137048e-05\n",
      "5.542163109180376e-05\n",
      "5.5285938757962394e-05\n",
      "5.51507702419296e-05\n",
      "5.501612278444035e-05\n",
      "5.488199364477031e-05\n",
      "5.474838010012528e-05\n",
      "5.461527944609912e-05\n",
      "5.4482688996174145e-05\n",
      "5.435060608154074e-05\n",
      "5.421902805130552e-05\n",
      "5.408795227188068e-05\n",
      "5.395737612713669e-05\n",
      "5.382729701833289e-05\n",
      "5.3697712363687256e-05\n",
      "5.356861959841808e-05\n",
      "5.3440016174716165e-05\n",
      "5.3311899561328535e-05\n",
      "5.318426724371106e-05\n",
      "5.3057116723709274e-05\n",
      "5.293044551947512e-05\n",
      "5.280425116535592e-05\n",
      "5.2678531211824975e-05\n",
      "5.255328322524566e-05\n",
      "5.242850478774652e-05\n",
      "5.2304193497276774e-05\n",
      "5.218034696714835e-05\n",
      "5.205696282628569e-05\n",
      "5.193403871892044e-05\n",
      "5.181157230438327e-05\n",
      "5.168956125710389e-05\n"
     ]
    }
   ],
   "source": [
    "def gradient_ascent(X,W,Y,alpha,stop):\n",
    "    m,n = X.shape\n",
    "    cost_old = np.inf\n",
    "    cost_new = cost(W,X,Y)\n",
    "    print(cost_old - cost(W,X,Y))\n",
    "    sum = 0\n",
    "    iter = 0\n",
    "\n",
    "    while(abs(cost_old-cost_new) > stop):\n",
    "        for j in range(0,n):\n",
    "            for i in range(0,m):\n",
    "                sum += (Y[i][0] - hyp(W,X[i])) * X[i][j]\n",
    "            W[0][j] = W[0][j] + (alpha/m) * sum\n",
    "        cost_old = cost_new\n",
    "        cost_new = cost(W,X,Y)\n",
    "        print(cost_old - cost(W,X,Y))\n",
    "        iter += 1\n",
    "\n",
    "    return W, iter  \n",
    "\n",
    "gradient_ascent(X_train,W,Y_train,0.04,0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Model Assestement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cschi\\Desktop\\Scuola\\Università\\Terzo anno\\Secondo Semestre\\Lab AI\\Progetto\\SilkIA\\my_algo\\my_logistic.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cschi/Desktop/Scuola/Universit%C3%A0/Terzo%20anno/Secondo%20Semestre/Lab%20AI/Progetto/SilkIA/my_algo/my_logistic.ipynb#ch0000002?line=3'>4</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mm \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(Y \u001b[39m==\u001b[39m Y_hat)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cschi/Desktop/Scuola/Universit%C3%A0/Terzo%20anno/Secondo%20Semestre/Lab%20AI/Progetto/SilkIA/my_algo/my_logistic.ipynb#ch0000002?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m accuracy, Y_hat\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cschi/Desktop/Scuola/Universit%C3%A0/Terzo%20anno/Secondo%20Semestre/Lab%20AI/Progetto/SilkIA/my_algo/my_logistic.ipynb#ch0000002?line=6'>7</a>\u001b[0m prediction(W,X_train,Y_train)[\u001b[39m0\u001b[39m],prediction(W,X_test,Y_test)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "def prediction(W,X,Y):\n",
    "    m = X.shape[0]\n",
    "    Y_hat = hyp(X, W) > 0.5\n",
    "    accuracy = 1.0/m * np.sum(Y == Y_hat)\n",
    "    return accuracy, Y_hat\n",
    "\n",
    "prediction(W,X_train,Y_train)[0],prediction(W,X_test,Y_test)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "702afc27125c4b44e8d6e4d91e2be92deaff276ecbbea6cc8c365578c4a94287"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
